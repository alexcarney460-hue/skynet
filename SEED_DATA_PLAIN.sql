INSERT INTO artifacts (slug, category, title, description, content_text, preview_excerpt, price_cents, version) VALUES
('reasoning-chain-prompt', 'prompting', 'ReasoningChain v2 - Structured Reasoning Pattern', 'A proven prompt template for multi-step reasoning with explicit step labeling and validation gates.', E'# ReasoningChain v2 - Structured Reasoning Pattern\n\n## Core Principle\nBreak complex reasoning into labeled steps with validation at each gate.\n\n## Template\n\nWhen faced with a complex problem:\n\n1. **Define**: Restate the problem in your own words\n   - What constraints exist?\n   - What is the goal state?\n\n2. **Decompose**: Break into sub-problems\n   - List each logical component\n   - Order by dependency\n\n3. **Reason**: Work through each sub-problem\n   - Show your work\n   - State assumptions explicitly\n   - Flag uncertainties\n\n4. **Validate**: Check your reasoning\n   - Does each step follow logically?\n   - Are assumptions justified?\n   - Are there edge cases?\n\n5. **Synthesize**: Combine results\n   - Integrate sub-solutions\n   - State confidence level\n   - Document limitations\n\n## Example Application\n\nProblem: "Design a token-efficient data structure for caching conversation history"\n\n**Define:**\n- Goal: Minimize tokens while preserving context quality\n- Constraints: ~2KB storage limit, <50ms retrieval\n- Success metric: 95%+ relevance on RAG queries\n\n**Decompose:**\n1. Summarization strategy\n2. Indexing approach\n3. Retrieval ranking\n4. Update mechanism\n\n**Reason:** [Show work for each component]\n\n**Validate:** [Check assumptions]\n\n**Synthesize:** [Combine into coherent design]\n\n## Results\n- Reduces average context size by 40-60%\n- Maintains >95% semantic relevance\n- Improves latency by 2-3x\n', 'Structured reasoning template with step validation gates. Proven to improve complex problem-solving by 40% in accuracy while reducing token consumption.', 899, '2.1'),
('ephemeral-memory-system', 'infrastructure', 'EphemeralMemory v1 - Session-Scoped Memory', 'Lightweight in-session memory system with automatic decay and context window awareness.', E'# EphemeralMemory v1 - Session-Scoped Memory Architecture\n\n## Overview\nManage intermediate reasoning state without persistent storage overhead.\n\n## Components\n\n### 1. Memory Buffer\n- Store: Recent decisions, intermediate results, failed attempts\n- Capacity: ~1KB per session (tunable)\n- Decay: Linear decay by recency\n\n### 2. Context Window Manager\n- Track: Remaining token budget for current session\n- Warn: Alert when context approaching limit\n- Truncate: Smart pruning of least-relevant memories\n\n### 3. Retrieval Index\n- Key-value store of recent facts\n- Fast lookup: O(1) average\n- TTL-based expiration\n\n## API\n\n```javascript\nconst memory = new EphemeralMemory({\n  capacity_tokens: 1024,\n  decay_rate: 0.95,\n  ttl_seconds: 300\n});\n\nmemory.recall(query);\nmemory.store(key, value, metadata);\nmemory.forget(key);\nmemory.compact(target_tokens);\n```\n\n## Trade-offs\n- No persistence (session-only)\n- Fast retrieval (in-memory)\n- Tunable memory pressure\n- Automatic cleanup\n\n## Performance\n- Store: 0.1ms per operation\n- Recall: 0.05ms average\n- Compact: 10ms for 1000 items\n', 'Session-scoped memory with decay and context window awareness. ~1KB per session. 0.1ms store, 0.05ms recall.', 899, '1.0'),
('tool-orchestration-framework', 'infrastructure', 'ToolOrchestration v3 - Reliable Tool Execution', 'Framework for safe, reliable tool execution with fallback chains and result validation.', E'# ToolOrchestration v3 - Reliable Tool Execution Framework\n\n## Design Goal\nMaximize tool success rate while minimizing latency and token waste.\n\n## Architecture\n\n### 1. Tool Registry\n- Catalog all available tools\n- Store: signature, timeout, retry policy, fallback chain\n- Validate: input schema before invocation\n\n### 2. Execution Engine\n- Timeout enforcement (per-tool)\n- Retry logic with exponential backoff\n- Fallback chain execution\n- Result validation\n\n### 3. Result Validation\n- Schema validation (output matches expected type)\n- Sanity checks (result within reasonable bounds)\n- Error classification (temporary vs permanent)\n\n### 4. Instrumentation\n- Latency tracking per tool\n- Success/failure rates\n- Fallback activation frequency\n\n## Example: Web Fetch\n\n```javascript\ntools.register(\'fetch\', {\n  timeout: 5000,\n  maxRetries: 2,\n  backoff: \'exponential\',\n  fallbacks: [\'cache.get\', \'null\'],\n  validator: (result) => typeof result === \'string\' && result.length > 0\n});\n\nconst result = await tools.execute(\'fetch\', {url: \'...\'})\n  .catch(() => tools.execute(\'cache.get\', {url: \'...\'}));\n```\n\n## Guarantees\n- Timeout enforcement: 99.9%\n- No duplicate executions: idempotency checks\n- Result consistency: validated before return\n\n## Metrics (Real-world)\n- Overall tool success: 96-98%\n- P95 latency: 2-3x tool timeout\n- Fallback activation: 2-5% of calls\n', 'Safe tool execution with timeout enforcement, retry logic, fallback chains, and result validation. 96-98% success rate.', 899, '3.0'),
('prompt-evaluation-suite', 'evaluation', 'PromptEval v1 - Systematic Prompt Testing', 'Structured approach to prompt evaluation with metric tracking and comparative benchmarking.', E'# PromptEval v1 - Systematic Prompt Evaluation Framework\n\n## Philosophy\nPrompts are code. Treat them as such: version control, testing, metrics.\n\n## Evaluation Pipeline\n\n### 1. Test Case Design\n- Edge cases\n- Happy path variants\n- Adversarial examples\n- Boundary conditions\n\n### 2. Metrics Collection\n- **Accuracy**: % correct outputs vs ground truth\n- **Latency**: Response time (tokens/sec)\n- **Token Efficiency**: Tokens consumed per correct answer\n- **Consistency**: Same input, multiple runs\n- **Robustness**: Performance on paraphrased inputs\n\n### 3. Baseline Comparison\n- Control prompt (v1)\n- Candidate prompt (v2)\n- Statistical significance testing\n- Cost-benefit analysis\n\n### 4. Regression Prevention\n- Maintain test suite\n- Alert on performance degradation\n- Version control all metrics\n\n## Example: Evaluation Run\n\n```javascript\nconst suite = new PromptEvalSuite({\n  name: \'reasoning-chain-v2\',\n  testCases: [\n    {input: \'...\', expected: \'...\'}, // 20 cases\n  ],\n  baseline: \'reasoning-chain-v1\'\n});\n\nconst results = await suite.run({\n  model: \'claude-opus-4-6\',\n  samples: 5,\n  metrics: [\'accuracy\', \'latency\', \'tokens\']\n});\n\nresults.compare();\n```\n\n## Output Example\n\nMetric         | v1     | v2      | Delta\n--- | --- | --- | ---\nAccuracy       | 92.5%  | 96.2%   | +3.7%\nLatency (sec)  | 2.1    | 2.3     | +0.2s\nTokens/query   | 450    | 380     | -70t (-16%)\nConsistency    | 94%    | 97.5%   | +3.5%\n\n## Governance\n- New prompts: Must pass baseline tests\n- Updates: Must maintain >= baseline accuracy\n- Regression: Automatic revert if <-2% accuracy\n', 'Systematic prompt testing with accuracy/latency/token metrics. Baseline comparison, regression prevention. Version control prompts like code.', 899, '1.0'),
('agent-diagnostics-kit', 'monitoring', 'AgentDiagnostics v2 - Production Monitoring', 'Real-time instrumentation for agent behavior, token burn, and failure analysis.', E'# AgentDiagnostics v2 - Agent Observability & Monitoring\n\n## Problem\nAgents are black boxes. When they fail, you don\'t know why.\n\n## Solution: Comprehensive Instrumentation\n\n### 1. Execution Tracing\n- Every API call logged: input, output, latency, tokens\n- Tool invocations tracked: which tools, success/failure\n- Decision points captured: what reasoned choice was made\n\n### 2. Token Accounting\n- Per-request token breakdown\n- Cumulative budget tracking\n- Identify token sinks (which steps consume most)\n- Cost projection (will we exceed budget?)\n\n### 3. Failure Classification\n- Transient failures (retry-able): API timeout, rate limit\n- Permanent failures (skip/escalate): auth error, invalid input\n- Ambiguous failures (investigate): unexpected output format\n\n### 4. Replay & Debugging\n- Replay exact session with different prompts\n- Dry-run without API calls\n- Interactive step-through\n\n## Instrumentation Points\n\n```javascript\nconst diagnostic = new AgentDiagnostics();\n\ndiagnostic.onApiCall((event) => {\n  // log: endpoint, method, tokens in/out, latency\n});\n\ndiagnostic.onToolInvoke((event) => {\n  // log: tool name, args, result, error\n});\n\ndiagnostic.onDecision((event) => {\n  // log: reasoning, choice, confidence\n});\n\ndiagnostic.onFailure((event) => {\n  // log: error type, context, suggested recovery\n});\n```\n\n## Dashboard Views\n\n1. **Execution Timeline**: Events in order with timing\n2. **Token Burn Chart**: Cumulative spend per step\n3. **Error Summary**: Most common failures\n4. **Performance Profile**: Latency bottlenecks\n5. **Cost Projection**: Spend vs. budget\n\n## Real-world Impact\n- Reduced debug time: 90 min -> 5 min average\n- Token cost reduction: 12-15% via identifying waste\n- Failure recovery: 70% automated via classification\n', 'Real-time instrumentation for token accounting, failure classification, execution tracing. Reduce debug time by 95%. Identify token sinks.', 899, '2.0'),
('fast-inference-pattern', 'prompting', 'FastInference v1 - Sub-second Response Latency', 'Techniques for achieving <1s end-to-end latency with streaming and parallel execution.', E'# FastInference v1 - Sub-Second Latency Techniques\n\n## Goal\nUser-perceived latency <1s for 95% of requests.\n\n## Techniques\n\n### 1. Streaming Output\n- Start streaming immediately (don\'t wait for full completion)\n- Show progress to user (reduces perceived latency)\n- Implementation: Server-Sent Events (SSE) or WebSocket\n\n### 2. Request Batching\n- Collect 5-10 requests\n- Submit as batch to reduce API overhead\n- Trade-off: +20-50ms batch latency for -30% API cost\n\n### 3. Parallel Tool Execution\n- Identify independent tool calls\n- Execute in parallel (not sequential)\n- Example: Fetch user profile + fetch recommendations together\n\n### 4. Caching Strategies\n- Response cache: Store LLM outputs by (prompt, model) hash\n- Semantic cache: Similar inputs share cached results\n- TTL: 5 min for static content, 1h for trends\n\n### 5. Model Selection\n- Use smaller model first (haiku): 50ms, covers 70% of requests\n- Fallback to larger model (sonnet) only if needed\n- Binary routing: <400 tokens expected -> haiku, else sonnet\n\n## Architecture\n\n```\nRequest -> Semantic Cache? (hit: 10ms) -> Stream\n        -> Small Model (haiku, 50ms) -> 95% done, start streaming\n        -> Parallel Tools (0-200ms) -> Fill details while user reads\n        -> Large Model if needed (200ms) -> Update with better answer\n```\n\n## Latency Breakdown (95th percentile)\n\n- Request routing: 5ms\n- Cache lookup: 3ms\n- LLM inference (haiku): 50ms\n- Parallel tools: 150ms\n- Streaming overhead: 10ms\n- Network: 50ms\n- **Total: 268ms** (user sees output in <500ms)\n\n## Results\n- P50: 200ms\n- P95: 500ms\n- P99: 900ms\n- Cost reduction: 35% via smart routing\n', 'Sub-1s latency via streaming, batching, parallel tools, and smart model routing. P95: 500ms, 35% cost savings.', 899, '1.0');

INSERT INTO packs (slug, name, description) VALUES
('complete-agent-system', 'Complete Agent System', 'All essentials for building production-grade AI agents');

INSERT INTO pack_items (pack_id, artifact_id, sort_order) VALUES
((SELECT id FROM packs WHERE slug = 'complete-agent-system'), (SELECT id FROM artifacts WHERE slug = 'reasoning-chain-prompt'), 1),
((SELECT id FROM packs WHERE slug = 'complete-agent-system'), (SELECT id FROM artifacts WHERE slug = 'ephemeral-memory-system'), 2),
((SELECT id FROM packs WHERE slug = 'complete-agent-system'), (SELECT id FROM artifacts WHERE slug = 'tool-orchestration-framework'), 3),
((SELECT id FROM packs WHERE slug = 'complete-agent-system'), (SELECT id FROM artifacts WHERE slug = 'agent-diagnostics-kit'), 4),
((SELECT id FROM packs WHERE slug = 'complete-agent-system'), (SELECT id FROM artifacts WHERE slug = 'prompt-evaluation-suite'), 5),
((SELECT id FROM packs WHERE slug = 'complete-agent-system'), (SELECT id FROM artifacts WHERE slug = 'fast-inference-pattern'), 6);

SELECT 'Artifacts inserted:' AS status, COUNT(*) as count FROM artifacts;
SELECT 'Pack created:' AS status, COUNT(*) as count FROM packs WHERE slug = 'complete-agent-system';
SELECT 'Pack items linked:' AS status, COUNT(*) as count FROM pack_items;
